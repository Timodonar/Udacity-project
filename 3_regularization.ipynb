{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (50000, 28, 28) (50000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (50000, 784) (50000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.655615\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 10.9%\n",
      "Minibatch loss at step 500: 3.214144\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 1000: 1.953369\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1500: 1.105249\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 2000: 0.947761\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2500: 0.720812\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 3000: 0.653069\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VOX1wPHvSUJISNgCIUDY952gYVEEg4LiiqJVVFo3\npNS2Wuz2a2uLWqvUulRbrUXAXSgo4q6IGhaVJWxlk0AStiAQdgIJ2c7vj3tjx5CQIZlkZjLn8zzz\nZOYu7z33zpsz733vJqqKMcaY0BDm7wCMMcbUHkv6xhgTQizpG2NMCLGkb4wxIcSSvjHGhBBL+sYY\nE0Is6ZuAJyJRIqIi0sbfsZwtEVkmIuOrMX+GiJzn45jqi0iuiLT2Zbke5T8lIpPc96NFZJsPyqxy\nzCLyoIj804vpnhWR26sWYfCwpO8DbmUsfZWISJ7H51uqUW61EoYJfqraWVW/rk4ZZeuRqp5S1VhV\n3VP9CE9bViJwPTDTl+V6G3N5PzKqOkVVf+bFYv4GTBGR8OrEGugs6fuAWxljVTUW2Alc5THsdX/H\nV1NEJMLfMVRXoK5DoMblhTuA+apa4O9Azpaqbgd2AZf5OZQaZUm/FohIuIj8UUQyReSAiLwuIk3c\ncTEiMltEDonIERFZLiJNReQJYCAw3d1jeKKcciNE5C0R2efO+4WIdPcYHyMiz4jILhE5KiKLSpOJ\niKS4LcCjIrJTRG52h3+vVSgik0Rkofu+tJvlJyKSAWxwh/9LRHaLyDERWSEiQ8rEOMVd92MislJE\nWorIDBH5S5n1WSAiPznDprxGRLaLSI6I/EUcDdxyu3qU00ZETpZu4zLLmCQin7u78oeB/3OH/1hE\ntrjfwwdui7V0nitEZKu7jf/uuY1EZKqITPeYtoeIFJUXvDsu1V1Gjoi8LCINPcbvFZFfichG4JjH\nsAvcOuS5R3nC/S5aiki8iHzklnlIRN4RkVbu/KfVIynTXSYicSLyhjt/loj8RkTEY3t95tajI+J0\nN408w3d0GbCoopEi0ldElrhl/VdELvMY18Jdj2PuNp5aTt0rjXmMiHwjIsfd+n2PiDQD3gY6eWyn\nZuV8R+XWfVcqcMUZ1i/4qaq9fPgCtgMjywz7LbAEaA1EAS8BL7rj7gXeBKKBCJx/0Bh33DJg/BmW\nFQH8CIh1y/0XsMxj/AxgAdASCAeGuX+7ALnAdW4Z8UD/8pYJTAIWuu+jAAU+AJoA0e7wHwFNgXrA\nH3BaS/XccX8E1rjLDAMGuPMOB7IAcadrDZwE4spZz9LlfuLO2xHILI0TpyvhwTLbe24F22wSUATc\n5W6LaOBGYDPQzV2Hh4Ev3OlbudvqSnfcb4BCj2VPBaZ7lN8DKPL4vMxj2h7ARUCk+50sA6Z6TLsX\nWOlui2iPYReUsx5PAgvddUgAxrjr0hh4B5hdXgxltmcb9/McYK5bj7q438stHtur0P2Ow4HJwPYz\n1MnjQF+Pz6OBbR7L3Qn80t2Wl7rbtqM7fj7wirse/YBvOb3ulcZ8EBjkvm8GDCi7PI8YvvuOOEPd\nd8ffDHzl7zxSky+/B1DXXpSf9LOAoR6fO+IkOAHuxmkZ9SmnrDMm/XKmbwmUuP8g9dx/1u7lTPcg\nMKuCMrxJ+uefIQZx1627+3kHcGkF02UCw9zPvwLmVVBm6XJTPIbdB3zgvr/Q8x8dWA9cXUFZk4D0\nMsO+KE1y7ufSbZcATMT9AXDHhQH7qULSLyeWccDXHp/3AjeXmea0pI+TgLdRzg+kO34I8O0ZvtPv\nEihQHygGOnmMvxf42GN7bfAYF+fO26Sc5Ya74zp4DPNM+qPc+iAe49/G2duKcutue49xj5dT90qT\n/n7gdqBhmRgqS/oV1n13/FXAJm//54LxZd07NczdTW4LfOju0h7BafmG4bRQZuAk/TfdLpJHxMsD\nSW7XyROlXSfANzjJtBlOCzUCyChn1rYVDPfWrjJx/M7tGjkKHMb5B23urntiectS5z/sFaC0K2k8\n8OpZLHcHTosYYDEQLiLniUgSzrp/5G38QHvgeY/vJwdnb6CNu4zvplfVEiC7kjjLJSKtRWSuiGS7\n39d0oHklsZUtYzDwBDBGVQ+5wxqKyEy3q+IYzt5d2XIr0hKnLu70GLYD53srtdfj/Un3b2zZglS1\nGKel37DsOFdrYKf73ZddVkucurvbY9yZtsUYnNb6Tre7buAZpvVUWd1vCBzxsqygZEm/hrkVPBu4\nSFWbeLyiVPWAOmcl/ElVe+B0efwApwUITsvmTG7HaT2NwNmt7+EOF5xd4yKgcznz7apgOMAJoIHH\n55blrVbpGxEZBfwcuBan6yUOyMNpzZWue0XLegW4XkTOxfln/KCC6Uq19XjfDtgDp/2A/BCna6Pw\nDOWU3a67gNvKfD/RqroKZzt+d6qoiITx/YTozfYq9Td3+j6q2giYgPNdnSm274hzuuJbwARV3egx\n6v/cGAe65V5Sptwz1aO9OC3sdh7D2lHFHzbgvzjdZOXZU2Y5nsvaixOn57ZtSwVU9WtVvRJnb2wB\n8EbpqEriO1PdB+gJrKukjKBmSb92PA9MFZG28N0Bq6vc9yNFpJebTI7hJOoSd759QKczlNsQyMfp\n34zB6YsGwE16rwBPi0iCeyDwAncv4lXgShG51t1biBeRfu6sa3EScZSI9ABuq2TdGuJ0heTg9FU/\nhNPSLzUdeEREOoljgLgHWFU1E9gEvAj8Rys/4+O3ItJYRDoAPwP+4zHuFeAG4Cb3/dl4Hrhf3IPg\n4hxIv84d9y4wWEQuF+cg+H04xy9KrQVGiEiiiDTFOZ5QkYY4/cnHRKSdW5ZXRCQSmAf8W1XfKafc\nk8AREWkO3F9mfIX1SFVP4XSxPCLOgf/OON07r3kbWxkf4nS3lWcJECYiv3Dr3SicH6g5qpoPvAc8\n6Na9Pjj966dx4xwnIo1w6t5xvv8/00JETtsTcZ2p7uPGfqa9xKBnSb92PIZz0O1zETkOfAWc445L\nxDnwdhznbJgP+V8yewr4kYgcFpHHyil3Bk6y3YvTj720zPh7cHZl1+D8MPwZpwW+DWf3+PfAISAN\n6O0Ra4Rb7jQq/+d/D6d7JQOnj/6AO2+pqTgt+M9xftSex+lHLvUy0JfKu3Zwy1nnxjvXMzZVzQC2\nAMdVdYUXZX1HVWcB/wTmud0ja3H2oFDVb3F+SJ5x160NzrY+5RHT+zg/XstwDkZW5E/ABcBRnET7\n1lmE2QkYjPPD53kWTwucvu/mON/xUpw65KmyevRj9+8OnO9pOlDVU41fwjnLKrLsCDexX4lzHv9B\nnIPRN7o//qVxtMapP9OBWfxvO5d1hxvvUZxjHD9yh6/D+aHe4XbXxZWJocK6LyLtcbr6KtvjDGql\nZ04Y4xcicgnwnKp28UFZb+AchHu40omrvowInB/Zq7SaF03VVSLyJM7B8uerWc7TQJSq/rjSiX1A\nRJ4FVqmqTy8sCzSW9I3feHRZLFbV8lqgZ1NWF2A10FNVq9ofXVHZl+HsnZ3COSX1VqCLF91R5iy4\nXTqKs9d0Hk6L+yZV/divgdUx1r1j/MI9y+YwTn/0s9Us6zGcLqyHfJ3wXaXXFOwHLgautYRfIxrj\ndBeewOm6e9gSvu9ZS98YY0KItfSNMSaEeJX0RWSyiGwUkQ0iMss9peoiEVntDntZKrhBlIjcKs59\nS7aKyK2+Dd8YY8zZqLR7R5wbTy0FeqlqnojMAT7GuZz5YlVNF5GHgB2qOqPMvHE4p0Ql4xygWQWc\nq6qHK1pe8+bNtUOHDlVeoRMnThATE1Pl+Y2pDqt/xl9WrVp1QFXjK5vO29u3RgDRIlKIc/XhCaBA\nVdPd8Z8Cv8M5b9zTpcCnHpeLf4pzb4xZFS2oQ4cOpKWleRnW6VJTU0lJSany/MZUh9U/4y8issOb\n6SpN+qqaLSKP49ybIw/nkuc5wGMikqyqaTgXW5R3yXQi379/xm6+f5l1abATcW5sRUJCAqmpqd7E\nXq7c3NxqzW9MdVj9M4Gu0qTvXlo+BufOkEdwroS8Bef+ME+JSH2cH4LiqgahqtNwrv4kOTlZq9NS\nspaW8SerfybQeXMgdySQpao57v1c5uHcWvdrVR2mqoNwLsNPL2febL6/B9CGqt/IyRhjTDV5k/R3\nAkPEeUKR4Fycstm95wduS/+3OPdUKesT4BL3BlZNcW6u9IlvQjfGGHO2Kk36qroc58lOq3FuNBWG\n0xXzaxHZjHMr1fdU9XMAEUkW99Fk7gHcP+M8DWglzhWTh2piRYwxxlTOq7N3VHUKMKXM4F+7r7LT\npuHcJ7z080ycx9kZY4zxM7si15g6Ytv+XPYfz/d3GCbAeXuevjEmQJ0qKubvC7fy70UZxNaP4JGx\nfbmyX+vKZzQhyVr6xgSxDdlHufofX/Kv1AyuHdCGTvGx/OyNNfxq7jpyTxX5OzwTgKylb0wQKigq\n4dkvtvHsF9uIi4lk5m3JXNQjgcLiEv7x2Vb++cU2Vm4/xNPjBpDUtom/wzUBxFr6xgSZb/Ye45pn\nv+Tpz7ZyVf/WLJg8nIt6JABQLzyM+y7pzuyJ51FUrFz3r6/45+dbKS6xW6gbhyV9Y4JEUbHTur/q\nH0vZfzyf58efy1M3JtGkwWmPo2VQxzg+vHcYl/dtxeML0rnphWVkH8nzQ9Qm0FjSNyYIbNt/nOue\n/5q/fbKFS3q1ZMHkCxndp+UZ52kcXY9nxiXxxA/6szH7KJf9fTHv/3dPLUVsApX16RsTwIpLlBlL\nM3l8QToxkeH88+YBZ3Vmjohw3bltSO7QlHtnr+Vnb6whdUsOD1zdm9j69u8fiuxbNyZAZR04wa/n\nriNtx2FG9UrgkWv7Et+wfpXKat8shrmTzuOZz7byrB3kDWnWvWNMgCkpUV76MovLnl5M+r7jPHVj\nf6b98NwqJ/xS9cLD+KUd5A151tI3JoDsOnSSX7+5jmWZh0jpHs/Usf1o2TjKp8soPch7//wNPL4g\nncVbD/DUjUkkNon26XJMYLKkb0yAmLd6N3+cvwER4bHr+vGD5DY4N7b1vdKDvCnd4vnTOxu47O+L\nT7uSt7hEySssJq+gmPzC4u/enyzzOa/Q/VxQTKf4WEb1SiAywjoRApUlfWMCwK5DJ/ntW/9lQNum\nPDWudlrd5R3kfei9TZwqKiGvsJiCopIqlRsXE8n157Zh3MC2dIqP9XHUpros6RsTAJ5amE6YCM/c\nNMDn3TmVKT3I++KXWWTsP0F0ZDhR9cJpEBlOdL1woty/0fXCiY4Mc8dFfDcsKjKM6HrhREaEsSzz\nELOW72Tm0iymLc5kcMc4bh7cjkt7tySqXnitrpcpnyV9Y/wsfd9x3l6TzV3DOtV6wi9VLzyMicM7\nV7ucC7vFc2G3ePYfz+fNVbuZvWIX985eS5MG9Rg7oA03DWpL14SGPojYVJUlfWP87IkFW4iJjOAn\nF1Y/6QaKFg2juDulC5OGd+brzIO8sWInry7bzswvs0hu35SbBrXj8r6tiI601n9ts6RvjB+t23WE\nTzbuY/LIbjSNOf12CsEuLEwY2qU5Q7s052DuKd5avZtZK3bxy7nreOC9jYwdkMi4Qe3o2aqRv0MN\nGZb0jfGjv32yhbiYSO4c1tHfodS4ZrH1mTi8M3cN68TyrEPMWrGTWSt38fLXO0hq24SbBrXlyn6t\nibErhWuUbV1j/OSrbQdYuu0A91/RM6RuiSAiDOnUjCGdmvHAiQLmrclm1oqd/Pat9Tzw7iYu6Z3A\nNQMSGdalORHhduqnr4VOTTMmgKgqf1uwhVaNoxg/pL2/w/GbpjGR3HlBR+4Y2oG0HYeZtzqbD9d/\nyztr99AsJpKr+rdmTFJrkto2qbFrFkKNJX1j/GDh5v2s2XmEqWP72qmMOK3/gR3iGNghjgeu7kXq\nlhzeWZvNGyt28tJX2+nQrAFjkhK5ZkAiHZvH+DvcoGZJ35haVlKiPP7JFjo2j+H6c9v4O5yAUz8i\nnEt7t+TS3i05ll/Ix+v3Mn9tNs98vpWnP9tK/7ZNuCapNVf2a13t+xGFIkv6xtSyd9ftYcu+4/zj\npgHWZ12JRlH1uGFgW24Y2JZvj+bx3ro9vL1mDw++t4mHP9jMBV2ac82A1lzSq6UdAPaSV1tJRCYD\nEwAF1gO3A0OBv+HcqTMXuE1Vt5WZrwOwGdjiDlqmqpN8EbgxwaiwuIQnP02nV6tGXNG3lb/DCSqt\nGkczcXhnJg7vTPq+48xfk807a/cw+T/riK63gUt6J3DjwLac16mZ9f+fQaVJX0QSgXuAXqqaJyJz\ngHHA74ExqrpZRO4G7gduK6eIDFVN8mHMxgStOWm72HnoJC/eNpCwMEtMVdUtoSG/Gd2DX13SnbQd\nh3l7zf8OAA9o14S7U7pwcY8Wto3L4e3+UAQQLSKFQANgD06rv/SKisbuMGNMBfILi3nms60kt29K\nSvd4f4dTJ4SFCYM6xjGoYxxTrurFm6t28/yiDO56JY3uCQ25e0RnrujbyrrRPIhq5Q9QEJF7gb8A\necACVb1FRIYB891hx4AhqnqszHwdgI1AujvN/aq6pJzyJwITARISEs6dPXt2lVcoNzeX2Fi7s5/x\njzPVv4+yCvnPlgJ+NyiK7nF2xk5NKS5Rlu8t5oPMArJzlfho4YpO9RiaGEG9OtzyHzFixCpVTa5s\nukqTvog0Bd4CbgSOAHOBN4GxwF9VdbmI/BrorqoTysxbH4hV1YMici7Oj0Tvsj8OnpKTkzUtLa2y\nuCuUmppKSkpKlec3pjoqqn/H8gsZ/tgX9G/ThJfvGFT7gYWgkhJl4eZ9PPvFNtbtPkpCo/rcNawT\nNw1qVycP+oqIV0nfm32ekUCWquaoaiEwD+cgbn9VXe5O8x/g/LIzquopVT3ovl8FZADdvFwHY+qM\n6UuyOHKykF9f2t3foYSMsDDhkt4tmf/Tobw+YTCd42N5+IPNDP3r5zy9cCtHThb4O0S/8Cbp7wSG\niEgDcQ6JXwxsAhqLSGkCH4Vzls73iEi8iIS77zsBXYFMn0RuTJA4mHuKGUsyuaJvK/okNvZ3OCFH\nxLnp2xt3DWHe3eeT3D6OpxamM3Tq5zzy4Wb2H8v3d4i1qtJ9HLf75k1gNVAErAGmAbuBt0SkBDgM\n3AEgIlcDyar6J2A48JB7ALgEmKSqh2pkTYwJUM+lZpBXWMzkUbaT62/ntGvK9FuT2bL3OP9K3cb0\nJZm89NV2fnBuGyZd2Jm2cQ38HWKN86pjS1WnAFPKDH7bfZWd9l3gXff9WzjHA4wJSXuO5PHqsh1c\nd04burSwEwwCRfeWDfn7uAFMHtWNfy/OZG7abmav3MV15yTy60t71Okrfe08JmNq0D8+3woK947s\n6u9QTDnaN4vhkWv7suS3I7jt/A68vSabi55I5cUvsygqrtozggOdJX1jakhmTi5z0nZz8+B2tGla\n97sNgllCoyj+eGUvPv7FcJLaNuHB9zZx5T+WsjzzoL9D8zlL+sbUkKcWbqV+RBg/HdHF36EYL3WO\nj+WVOwbx/PhzOZ5fxI3TlnHv7DXsq0MHey3pG1MDNu45ynvr9nDH0I51un+4LhIRRvdpycL7LuSe\ni7vy0Ya9XPR4KtMWZ1BQFPxdPpb0jcG5kOfRjzbz2MffkL7veLXLe2JBOo2j63HX8E4+iM74Q3Rk\nOPeN6sank4dzXudmPPLhN1z29GKWbj3g79Cqpe5dlmZMFTz2yRb+vSiTMHFOsezZqhHXDmjN1f0T\nadk46qzKStt+iM+/2c9vR/egcXS9GorY1Jb2zWKYfutAPv9mHw++t4nxM5ZzWZ+W3H9lLxKbRPs7\nvLNmSd+EvLlpu3h+UQa3DG7H5FHdeH/dHuav3cMjH37Dox99w5COzbh2QCKj+7akUdSZk7iq8tgn\nW4hvWJ/bzu9QOytgasVFPRI4v3NzXlicybOp2/hiy35+NqILdw3vRP2I4LmXklc3XKtNdu8dU5tW\nZB3ilunLGNQxjpduH0Q9j7sxZh04wTtrs5m/JpvtB08SGRHGyJ4tGJOUSEr3+HL/0f8xdyFPrDrF\nn8f05ofndajFNTG1affhkzz8/mY+3riXDs0aMOWq3ozo0cKvMXl77x1L+iZk7Tx4kjHPLqVpg0je\nvnsojRuU34pXVdbtPsr8Ndm8t24PB08U0Di6Hpf3bcU1Sa0Z2CGOsDChpEQZMfVjSiLq89l9KURG\n2CGzum5xeg4PvLeRzJwTjOzZgl9d2p3uCQ398hAXb5O+de+YkHQsv5A7Xl5JicKM2wZWmPDBOZsj\nqW0Tkto24Q9X9GTptgO8s8bZA5i1YieJTaK5Oqk1cQ0i2XGshCdv6GYJP0QM7xbPx/cOZ+aXWTzz\n2VZG/30JrRtHMbxbPMO7xTO0S/OAO65jSd+EnKLiEn72xhq2HzjBq3cOpmPzGK/nrRcexojuLRjR\nvQUnThXx6aZ9zF+bzbTFmRSXKK1jhTFJiTUYvQk0kRFhTLqwM2PPSeTTTftYnJ7DB//9ltkrdxEm\nkNS2CRd2a8Hwbs3p16YJ4X6+p78lfRNy/vz+Jhan5zB1bF/O69ysyuXE1I/gmgGJXDMgkZzjp/h0\n0z6K92/z+z+18Y8WDaO4ZXB7bhncnsLiEtbuOsLi9BwWp+fw98/SeWphOk0a1GNol+Zc2NXZEzjb\nM8N8wZK+CSmvfr2dl7/ewYQLOjJuUDuflRvfsD43D25HaqrdOdw4e4QDO8QxsEMcv7ykO4dOFLBk\naw6L0w+wZKuzJwDQLSGW4e4PwKCOcUTVq/mzgCzpm5CxZGsOD7y3iYt7tOB3l/f0dzgmhMTFRDIm\nKZExSYmoKt/sPe7sBWzN4ZWvdzB9aRZR9cIY3bslfx83oEZjsaRvQsK2/bnc/fpquraI5embBlgX\njPEbEaFnq0b0bNWIH1/YmZMFRSzPPMSi9Bzq18IJAJb0TZ13+EQBd768kvoRYUy/NZnYOvh8VBO8\nGkRGMKJHi1o7z99qv6nTCopK+PFrq/j2aD6z7hpitzg2Ic9OJjZ1lqpy//z1rMg6xGPX9ePc9k39\nHZIxfmdJ39RZLyzJZE7abn5+UReuGWDnzhsDlvRNHfXppn08+tE3XNG3FZNH2gPJjSllSd/UOZv2\nHOPe2Wvom9iYx3/QnzA7U8eY71jSN3XK/uP5THh5JY2i6jH9R8lERwbPLW+NqQ129o6pM/ILi5n4\nyioOnyxk7qTzaNGo9i9xNybQedXSF5HJIrJRRDaIyCwRiRKRi0VktYisFZGlIlLu059F5Hcisk1E\ntojIpb4N35j/+f289azddYSnbkyiT2Jjf4djTECqNOmLSCJwD5Csqn2AcGAc8C/gFlVNAt4A7i9n\n3l7utL2B0cBzImL728bn1u8+yrw12fxsRBdG92np73CMCVje9ulHANEiEgE0APYACjRyxzd2h5U1\nBpitqqdUNQvYBgyqXsjGnO651G00jIrgxxfag8iNOZNK+/RVNVtEHgd2AnnAAlVdICITgA9FJA84\nBgwpZ/ZEYJnH593usO8RkYnARICEhARSU1PPdj2+k5ubW635TfDZk1vCxxvyuLJzPVYt+9KvsVj9\nM4Gu0qQvIk1xWuwdgSPAXBEZD4wFLlfV5SLya+BJYEJVglDVacA0cB6XWJ3HHdrjEkPPL+esI6re\ntzx4cwpxMZF+jcXqnwl03nTvjASyVDVHVQuBecBQoL+qLnen+Q9wfjnzZgNtPT63cYcZ4xO7D5/k\nnbXZjBvU1u8J35hg4E3S3wkMEZEG4jzt92JgE9BYREovdRwFbC5n3neBcSJSX0Q6Al2BFT6I2xgA\nXliciQjcNcz68o3xhjd9+stF5E1gNVAErMHpitkNvCUiJcBh4A4AEbka50yfP6nqRhGZg/MjUQT8\nVFWLa2ZVTKjJOX6K2St3MXZAG1o3ifZ3OMYEBa8uzlLVKcCUMoPfdl9lp30Xp4Vf+vkvwF+qEaMx\n5Zr5ZRaFxSVMSuns71CMCRp2GwYTlI7mFfLq1zu4rG8rOjaP8Xc4xgQNS/omKL22bAe5p4q421r5\nxpwVS/om6OQVFDNjaRYjusfTu7XdbsGYs2FJ3wSd2St3cuhEAT8dUe7tnowxZ2BJ3wSVgqISpi3O\nZFCHOJI7xPk7HGOCjiV9E1Tmr83m26P53D3C+vKNqQpL+iZoFJcoz6dm0Lt1Iy7sFu/vcIwJSpb0\nTdD4eMNeMg+c4KcjuuBcHG6MOVuW9E1QUFWe/WIbnZrHcGlvu1++MVVlSd8EhUXpOWz69hiTUjoT\nbg86N6bKLOmboPDcFxm0bhzFNUmnPY7BGHMWLOmbgLci6xArth/iruGdiIywKmtMddh/kAl4z6Vu\nIy4mknED2/k7FGOCniV9E9A27jlK6pYc7rygI9GR4f4Ox5igZ0nfBLTnUjNoWD+C8UPa+zsUY+oE\nS/omYGXm5PLh+m8Zf157GkfX83c4xtQJlvRNwHp+UQaR4WHcMbSjv0Mxps6wpG8C0p4jeby9Jptx\nA9sS37C+v8Mxps6wpG8C0gtLMlGFu4bbA8+N8SVL+ibgHMw9xawVOxmTlEibpg38HY4xdYolfRNw\nXvxyO6eKSvhJirXyjfE1S/omoBzPL+Tlr7czundLurRo6O9wjKlzIryZSEQmAxMABdYDtwOfAqX/\nlS2AFap6TTnzFrvzAOxU1aurG7Spu15btpPj+UXcnWKPQjSmJlSa9EUkEbgH6KWqeSIyBxinqsM8\npnkLeKeCIvJUNckn0Zo6Lb+wmBlLMxnWtTl929gDz42pCd5270QA0SISATQA9pSOEJFGwEXAfN+H\nZ0LJnLRdHMi1B54bU5MqTfqqmg08DuwEvgWOquoCj0muAT5T1WMVFBElImkiskxETuv+MQacB57/\ne1Em57ZvyuCO9sBzY2qKN907TYExQEfgCDBXRMar6mvuJDcB089QRHtVzRaRTsDnIrJeVTPKLGMi\nMBEgISGB1NTUs18TV25ubrXmN/6RuquQ7CMF3Ni5hEWLFvk7nCqz+mcCnajqmScQ+QEwWlXvdD//\nCBiiqneLSHNgC5CoqvmVLkzkJeB9VX2zommSk5M1LS3tLFbh+1JTU0lJSany/Kb2FRSVMOLxVOIb\n1uftu89hiICFAAATjklEQVQP6uffWv0z/iIiq1Q1ubLpvOnT3wkMEZEG4vw3Xgxsdsddj5PEy034\nItJUROq775sDQ4FN3qyACR1z0naRfSSPyaO6BXXCNyYYeNOnvxx4E1iNc+plGDDNHT0OmOU5vYgk\ni0hpd09PIE1E1gFfAFNV1ZK++c6pomKe/WIb57RrwvCuzf0djjF1nlfn6avqFGBKOcNTyhmWhnNO\nP6r6FdC3eiGaumzOyl18ezSfx67vZ618Y2qBXZFr/Ca/sJhnv8gguX1TLuhirXxjaoMlfeM3/1m5\ni73H8q0v35haZEnf+EV+YTHPpW5jUIc4zu/czN/hGBMyLOkbv5i1Yif7jp3iF6O6WivfmFpkSd/U\nOqeVn8HgjnGc39n68o2pTZb0Ta17bdkOco6fYvKobv4OxZiQY0nf1Kq8gmKeX5TJeZ2aMaST9eUb\nU9ss6Zta9dqyHRzItVa+Mf5iSd/UmpMFRTy/KIOhXZoxyO6kaYxfWNI3tebVr3dw8EQBk0daK98Y\nf7Gkb2rFiVNF/Hux81Ss5A7WyjfGXyzpm1rxytc7OHSigF9YK98Yv7Kkb2pc7qkipi3O4MJu8Zzb\nvqm/wzEmpFnSNzXu5a+2c/hkoZ2xY0wAsKRvatTx/EJeWJLJiO7xJLVt4u9wjAl5lvRNjXr5q+0c\nOVloffnGBAhL+qbGHMsv5IUlWVzcowX9rZVvTECwpG9qzEtfbudonrXyjQkklvRNjTiaV8j0JZmM\n7JlA3zaN/R2OMcZlSd/UiBe/zOJYfhG/GNnV36EYYzxY0jc+dzSvkBlLs7ikVwJ9Eq2Vb0wgsaRv\nfG7G0iyO5xdZX74xAciSvvGpoycLeXFpFqN7t6RX60b+DscYU4ZXSV9EJovIRhHZICKzRCRKRJaI\nyFr3tUdE5lcw760istV93erb8E2gmb40k+OnirjX+vKNCUgRlU0gIonAPUAvVc0TkTnAOFUd5jHN\nW8A75cwbB0wBkgEFVonIu6p62FcrYALH4RMFzFyaxeV9W9KzlbXyjQlE3nbvRADRIhIBNAD2lI4Q\nkUbARUB5Lf1LgU9V9ZCb6D8FRlcvZBOIiopL+NeiDE4WFnPvxdaXb0ygqrSlr6rZIvI4sBPIAxao\n6gKPSa4BPlPVY+XMngjs8vi82x1mgtTJgiIyc06QkZPLtv3OKyMnl+0HTlJQXMKV/VrRvWVDf4dp\njKmAN907TYExQEfgCDBXRMar6mvuJDcB06sThIhMBCYCJCQkkJqaWuWycnNzqzW/cRwrUL7NLeHb\nEyXsyS1hzwnn88F8/W4aAVo0EFrHhjGyXTitYiIY1OJoSG9/q38m0FWa9IGRQJaq5gCIyDzgfOA1\nEWkODAKurWDebCDF43MbILXsRKo6DZgGkJycrCkpKWUn8VpqairVmT9UFRaX8PeF6SzPPERGTi6H\nTxZ+Ny66Xjid4mO4oEcsXeJj6dwili4tYmnfrAH1I8L9GHXgsfpnAp03SX8nMEREGuB071wMpLnj\nrgfeV9X8Cub9BHjE3VsAuAT4XTXiNTWgsLiEn7+xho837mVgh6aM7tOKLi1i6RwfQ5cWsbRuHE1Y\nmPg7TGOMD3jTp79cRN4EVgNFwBrcVjkwDpjqOb2IJAOTVHWCqh4SkT8DK93RD6nqIZ9Fb6qtoKiE\nn89azScb9/GnK3txxwUd/R2SMaYGedPSR1Wn4Jx6WXZ4SjnD0oAJHp9nAjOrHqKpKQVFJfz0jdV8\numkfD1zVi9uGWsI3pq7zKumbuudUUTE/fX01Czfv56ExvfnReR38HZIxphZY0g9Bp4qKufu11Xz2\nzX7+PKY3P7SEb0zIsKQfYvILi/nJa6v4YksOD1/Th/FD2vs7JGNMLbKkH0LyC4uZ9NoqUrfk8Mi1\nfbl5cDt/h2SMqWWW9ENEfmExE19dxZKtOUwd25dxgyzhGxOKLOmHgPzCYu56JY2l2w7w17H9uGFg\nW3+HZIzxE0v6dVxegZPwv8w4wGPX9eMHyZbwjQlllvTrsLyCYia8spKvMg7y+PX9ue7cNv4OyRjj\nZ5b066iTBUXc+VIay7MO8uQN/bl2gCV8Y4wl/TrpZEERd7y0khVZh3jyhiSuGWB3szbGOCzp1zEn\nThVx+0srSdt+iKduTGJMkiV8Y8z/WNKvQ3JPFXH7iytYvfMIT48bwFX9W/s7JGNMgLGkX0ecLPBM\n+Elc2c8SvjHmdJb064BTRcX8+NVVrNpxmH/cdA5X9Gvl75CMMQHKkn6QKy5RJv9nLUu2HuCx6/tZ\nwjfGnFGYvwMwVaeq/H7eej5cv5f7r+jJDXbhlTGmEpb0g5Sq8siHm/lP2i7uuagLE4Z18ndIxpgg\nYEk/SD37xTZeWJLFbed3YPKobv4OxxgTJCzpB6FXvt7O4wvSGTsgkT9d2QsRe2i5McY7lvSDzPw1\n2fzpnY2M7JnAX6/vR1iYJXxjjPcs6QeRhZv28cu56zivUzP+efMA6oXb12eMOTuWNYLE1xkHufuN\n1fRp3YgXbk0mql64v0MyxgQhS/pB4L+7jzDh5ZW0j2vAS7cPIra+XV5hjKkar5K+iEwWkY0iskFE\nZolIlDj+IiLpIrJZRO6pYN5iEVnrvt71bfh139Z9x7l15gqaxkTy6p2DaRoT6e+QjDFBrNImo4gk\nAvcAvVQ1T0TmAOMAAdoCPVS1RERaVFBEnqom+SziELLr0El+OGMFEeFhvD5hMC0bR/k7JGNMkPO2\nnyACiBaRQqABsAd4GLhZVUsAVHV/zYQYmvYfz2f8jOWcLChizqTzaN8sxt8hGWPqgEqTvqpmi8jj\nwE4gD1igqgtEZBZwo4hcC+QA96jq1nKKiBKRNKAImKqq88tOICITgYkACQkJpKamVnmFcnNzqzV/\nIDhRqDy6PI/9ecpvkqPY+81q9n7j76iMN+pC/TN1mzfdO02BMUBH4AgwV0TGA/WBfFVNFpGxwExg\nWDlFtHd/ODoBn4vIelXN8JxAVacB0wCSk5M1JSWlyiuUmppKdeb3t5MFRYyfvpz9efnMuG0gw7rG\n+zskcxaCvf6Zus+bA7kjgSxVzVHVQmAecD6w230P8DbQr7yZVTXb/ZsJpAIDqhlznVV6i+S1u47w\nzE1JlvCNMT7nTdLfCQwRkQbiXO9/MbAZmA+McKe5EEgvO6OINBWR+u775sBQYJMvAq9riopL+MVs\n5xbJU6/rx+g+dotkY4zvedOnv1xE3gRW4/TLr8HpiokGXheRyUAuMAFARJKBSao6AegJ/FtESnB+\nYKaqqiX9MopLlF/NXcdHG+wWycaYmuXV2TuqOgWYUmbwKeCKcqZNw/0BUNWvgL7VjLFOKylx7ok/\nf+0efn1pd7tFsjGmRtkVuX6kqkx5d+N398T/6Ygu/g7JGFPHWdL3E1Xl4Q828+qyHfx4eCe7J74x\nplZY0vcDVeVvn2xhxlLnISj/d1kPuye+MaZWWNL3g2c+28ZzqRncPLgdU66yh6AYY2qPJf1a9q/U\nDJ5amM7157bh4TF9LOEbY2qVJf1aNHNpFn/9+Buu6t+av15nT70yxtQ+S/q15LVlO3jo/U2M7t2S\nJ2/oT7glfGOMH1jSrwVz0nZx//wNXNyjBc/cZI85NMb4j2WfGvbO2mx++9Z/Gda1Oc/ecg6REbbJ\njTH+YxmoBn20/lvum7OOwR3jmPZDe66tMcb/LOnXkIWb9vHzWWtIatuEGbcOJDrSEr4xxv8s6deA\nRek53P36anq3bsSLtw8kxh5kbowJEJb0feyrjANMfCWNLi1ieeWOwTSKqufvkIwx5juW9H1o5fZD\n3PlSGu2bNeC1CYNp3MASvjEmsFjS95E9R/K448WVtGoSxesThhAXE+nvkIwx5jR1KulvyD5KUYnW\n+nJVlT+9s4GiEuWl2wYR37B+rcdgjDHeqDNJPyMnlzHPfsm7GYW1vuyPN+xl4eb9TB7VlXbNGtT6\n8o0xxlt1Jul3jo9lTFJr3s8sZN2uI7W23GP5hUx5dyO9WjXijqEda225xhhTFXUm6QNMuao3jSOF\n++asJb+wuFaW+bePt3Ag9xSPju1LhN1ewRgT4OpUlmocXY87+0aSkXOCxz/ZUuPLW7XjMK8t38Gt\n53egf9smNb48Y4yprjqV9AH6NI9g/JB2zPgyi+WZB2tsOYXFJfx+3npaNoril5d0r7HlGGOML9W5\npA/wu8t60rZpA3715jpyTxXVyDKmLc5ky77j/HlMH2LtiltjTJCok0k/pn4ET9zQn92H83jkw80+\nL3/7gRM889lWLuvTkpG9EnxevjHG1BSvkr6ITBaRjSKyQURmiUiUOP4iIukisllE7qlg3ltFZKv7\nutW34VdsYIc47hrWiTeW72RReo7PylVV/jB/PZHhYTxwdW+flWuMMbWh0qQvIonAPUCyqvYBwoFx\nwG1AW6CHqvYEZpczbxwwBRgMDAKmiEhTn0VfiftGdaNri1h+8+Y6jp70zfn7b6/J5sttB/nNZT1I\naBTlkzKNMaa2eNu9EwFEi0gE0ADYA/wEeEhVSwBUdX85810KfKqqh1T1MPApMLr6YXsnql44T96Q\nxIHcAh54b2O1yzt0ooCHP9jMOe2acMugdj6I0BhjalelRyBVNVtEHgd2AnnAAlVdICKzgBtF5Fog\nB7hHVbeWmT0R2OXxebc77HtEZCIwESAhIYHU1NSqrAsAubm5p81/ZccI3l6TTaIeILll1Q+6Tl9/\niqMnixibFM7ixYuqXI6pu8qrf8YEkkozoNsdMwboCBwB5orIeKA+kK+qySIyFpgJDKtKEKo6DZgG\nkJycrCkpKVUpBoDU1FTKzj90WAkZz33JG1vzue3K82gee/b3xvlq2wGWfrycu1M6M350jyrHZ+q2\n8uqfMYHEm+6dkUCWquaoaiEwDzgfp9U+z53mbaBfOfNm4/T7l2rjDqtV9cLDePKGJHLzi/j9vPWo\nnt1N2fILi/nD/A20b9aAey7uWkNRGmNMzfMm6e8EhohIAxER4GJgMzAfGOFOcyGQXs68nwCXiEhT\nd4/hEndYreuW0JBfXtKNBZv28faas/vdefaLbWQdOMEj1/a159waY4JapUlfVZcDbwKrgfXuPNOA\nqcB1IrIeeBSYACAiySIy3Z33EPBnYKX7esgd5hcThnUiuX1Tpry7kW+P5nk1T/q+4zy/KIOxAxIZ\n2qV5DUdojDE1y6uzd1R1iqr2UNU+qvpDVT2lqkdU9QpV7auq56nqOnfaNFWd4DHvTFXt4r5erKkV\n8UZ4mPD4D/pTVKz85s3/VtrNU1Ki/G7eemLrR/CHK3rWUpTGGFNz6uQVuWfSoXkMv7+8B0u2HuD1\n5TvPOO2slTtZteMwf7iiF82qcPDXGGMCTcglfYDxQ9ozrGtzHvlwMzsOnih3mv3H8pn60Tec37kZ\n151z2lmmxhgTlEIy6YsIf72uH+Fhwq/mrqO4nEcsPvjeJk4VlfCXa/viHL82xpjgF5JJH6B1k2ge\nuKo3K7cfZubSrO+N+/ybfXyw/lvuuagLHZvH+ClCY4zxvZBN+gBjz0lkVK8E/rZgC1v3HQfgxKki\n/jh/I90SYpk4vLOfIzTGGN8K6aQvIjxybV9i60dw35x1FBaX8NSn6WQfyePRsX2JjAjpzWOMqYNC\nPqvFN6zPw9f0YX32UX45Zx0zv8zilsHtOLd9nL9DM8YYnwv5pA9wed9WjElqzbvr9tAstj6/sXvr\nGGPqKHvOn+vBq3tz5GQhtw/tQOPoev4OxxhjaoQlfVeTBpG8fMcgf4dhjDE1yrp3jDEmhFjSN8aY\nEGJJ3xhjQoglfWOMCSGW9I0xJoRY0jfGmBBiSd8YY0KIJX1jjAkhUtkjA2ubiBwFtp5hksbA0TOM\nbw4c8GlQtauy9Qv05VW3vLOd/2ym92ba6k5j9c+/y6vt+nc28/hquorGt1fV+EpLV9WAegHTqjk+\nzd/rUJPrH+jLq255Zzv/2UzvzbTVncbqn3+XV9v172zm8dV01V3HQOzeea+a44Ndba+fr5dX3fLO\ndv6zmd6baX01TbCy+ldz8/hqumqtY8B171SXiKSparK/4zChyeqfCXSB2NKvrmn+DsCENKt/JqDV\nuZa+McaYitXFlr4xxpgKWNI3xpgQYknfGGNCSEglfRGJEZE0EbnS37GY0CMiPUXkeRF5U0R+4u94\nTGgKiqQvIjNFZL+IbCgzfLSIbBGRbSLyf14U9VtgTs1EaeoyX9RBVd2sqpOAG4ChNRmvMRUJirN3\nRGQ4kAu8oqp93GHhQDowCtgNrARuAsKBR8sUcQfQH2gGRAEHVPX92one1AW+qIOqul9ErgZ+Aryq\nqm/UVvzGlAqKB6Or6mIR6VBm8CBgm6pmAojIbGCMqj4KnNZ9IyIpQAzQC8gTkQ9VtaQm4zZ1hy/q\noFvOu8C7IvIBYEnf1LqgSPoVSAR2eXzeDQyuaGJV/QOAiNyG09K3hG+q66zqoNvwGAvUBz6s0ciM\nqUAwJ/0qUdWX/B2DCU2qmgqk+jkME+KC4kBuBbKBth6f27jDjKktVgdN0AnmpL8S6CoiHUUkEhgH\nvOvnmExosTpogk5QJH0RmQV8DXQXkd0icqeqFgE/Az4BNgNzVHWjP+M0dZfVQVNXBMUpm8YYY3wj\nKFr6xhhjfMOSvjHGhBBL+sYYE0Is6RtjTAixpG+MMSHEkr4xxoQQS/rGGBNCLOkbY0wIsaRvjDEh\n5P8BHO7sRZxdsz4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65d0835e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 671.527954\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 33.8%\n",
      "Minibatch loss at step 500: 194.701706\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 1000: 124.661629\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1500: 69.011688\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2000: 41.541546\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 2500: 25.260809\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 15.439947\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 85.6%\n",
      "Test accuracy: 92.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65cf2a8a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 420.091095\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 31.3%\n",
      "Minibatch loss at step 2: 1892.247803\n",
      "Minibatch accuracy: 39.8%\n",
      "Validation accuracy: 33.2%\n",
      "Minibatch loss at step 4: 490.270264\n",
      "Minibatch accuracy: 42.2%\n",
      "Validation accuracy: 48.7%\n",
      "Minibatch loss at step 6: 132.108521\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 8: 106.862633\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 10: 62.723511\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 12: 17.332724\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 14: 26.974688\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 16: 2.436239\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 18: 0.164397\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 20: 2.404567\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 22: 0.062851\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 24: 0.000266\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 26: 0.000511\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 30: 0.000005\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 36: 0.000005\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 42: 0.000004\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 48: 0.000004\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 54: 0.000003\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 60: 0.000003\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 66: 0.000003\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 72: 0.000003\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 78: 0.000003\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 84: 0.000002\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 90: 0.000002\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 96: 0.000002\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.6%\n",
      "Test accuracy: 80.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 446.635284\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 35.8%\n",
      "Minibatch loss at step 2: 662.948608\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 4: 60.643791\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 56.0%\n",
      "Minibatch loss at step 6: 17.805992\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 8: 3.005082\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 10: 0.885762\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 12: 4.565117\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 14: 3.039821\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 18: 2.639697\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 20: 2.144369\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 22: 2.754564\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 24: 0.526611\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 26: 0.062185\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 28: 0.515288\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 30: 1.662982\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 34: 0.238799\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 40: 1.863310\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 42: 1.297476\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 48: 0.622075\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 52: 0.043938\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 54: 0.404934\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 58: 0.886358\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 64: 0.000071\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 66: 1.806568\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 70: 0.713273\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 72: 0.342007\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 80: 0.696542\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 82: 0.016339\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 86: 0.085171\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 88: 0.421432\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.4%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.4%\n",
      "Minibatch loss at step 100: 1.567037\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 69.5%\n",
      "Test accuracy: 76.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.262029\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 34.3%\n",
      "Minibatch loss at step 500: 1.052426\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 1000: 0.856516\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 1500: 0.753074\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2000: 0.500711\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2500: 0.447024\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 3000: 0.406440\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 3500: 0.430192\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 4000: 0.383734\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 4500: 0.348701\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 5000: 0.359585\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 5500: 0.338011\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 6000: 0.342821\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 6500: 0.334695\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 7000: 0.360944\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 7500: 0.353923\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 8000: 0.323019\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 8500: 0.344376\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 9000: 0.341051\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 88.7%\n",
      "Test accuracy: 94.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.341298\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 39.5%\n",
      "Minibatch loss at step 500: 0.450487\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 1000: 0.364010\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 1500: 0.443223\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 2000: 0.134278\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2500: 0.126326\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 3000: 0.126418\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 3500: 0.156336\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 4000: 0.126767\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 4500: 0.085241\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 5000: 0.098912\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 5500: 0.035340\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 6000: 0.050827\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 6500: 0.057607\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 7000: 0.027715\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 7500: 0.023367\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 8000: 0.012408\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 8500: 0.002286\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 9000: 0.001509\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 9500: 0.003443\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 10000: 0.012044\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 10500: 0.003581\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 11000: 0.016483\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 11500: 0.047808\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 12000: 0.000325\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 12500: 0.001224\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 13000: 0.000383\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 13500: 0.000330\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 14000: 0.000197\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 14500: 0.000823\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 15000: 0.000391\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 15500: 0.014898\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 16000: 0.027567\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 16500: 0.000211\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 17000: 0.000192\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 17500: 0.000362\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 18000: 0.000850\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Test accuracy: 94.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "  logits = tf.matmul(drop3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.855153\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 22.3%\n",
      "Minibatch loss at step 500: 0.712385\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 1000: 0.621890\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 1500: 0.614579\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 2000: 0.386003\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 2500: 0.357819\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 3000: 0.444942\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 3500: 0.387340\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 4000: 0.437404\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 4500: 0.349817\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 5000: 0.438859\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 5500: 0.324692\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 6000: 0.421333\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 6500: 0.417050\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 7000: 0.360152\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 7500: 0.397382\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 8000: 0.257997\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 8500: 0.366009\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 9000: 0.483889\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 9500: 0.286414\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 10000: 0.286468\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 10500: 0.350567\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 11000: 0.252288\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 11500: 0.488922\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 12000: 0.188026\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 12500: 0.367094\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 13000: 0.247945\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 13500: 0.350465\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 14000: 0.225227\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 14500: 0.276476\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 15000: 0.135393\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 15500: 0.175177\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 16000: 0.239054\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 16500: 0.188453\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 17000: 0.182370\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 17500: 0.182467\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 18000: 0.115226\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 18500: 0.108388\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 19000: 0.266931\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 19500: 0.154759\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 20000: 0.205796\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.1%\n",
      "Test accuracy: 94.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
